---
title: "homework 3"
uniqname: "mchenran"
format: 
  html:   
    code-fold: true
    code-summary: "Show the code"
    embed-resources: true
---

The link to my github repository: [homework 3](https://github.com/snuuym/stats506/tree/main)

## Problem 1 

### a.   
Download the file AUX_I, and determine how to read it into R. Then download the file DEMO_I. Note that each page contains a link to a documentation file for that data set. Merge the two files to create a single data.frame. Keep only records which matched. Print out the dimensions of the merged data.frame.
```{r}
# chatgpt taught me how to read the .xpt file
library(haven)
aux <- read_xpt("AUX_I.xpt")
demo <- read_xpt("DEMO_I.xpt")
data_merged <- merge(x = aux, y = demo, by = "SEQN") #inner join
dim(data_merged)
```
### b. 
We’ll be using the following variables. Clean up each - ensure all missing values are actually NA (rather than 999 or something), and if it’s categorical, convert it to factor with. informative levels. 

Gender
Citizenship status
Number of children 5 years or younger in the household
Annual household income -  There’s also an issue with the ordering of the categories here; take a look, identify the issue, and implement a solution.

```{r}
unique(df_clean$INDHHIN2)
```

```{r}
library(dplyr)

#RIAGENDR
#DMDCITZN
#DMDHHSZA
#INDHHIN2 with ordering problem

df_clean <- data_merged %>%
  mutate(
    # ---- Gender ----
    RIAGENDR = case_when(
      RIAGENDR %in% c(9, 99, 999) ~ NA, 
      TRUE ~ as.character(RIAGENDR)
    ),

    # ---- Citizenship ----
    DMDCITZN = case_when(
      DMDCITZN %in% c(7,9, 99, 999) ~ NA,   
      TRUE ~ as.character(DMDCITZN)
    ),

    # ---- Children under 5 ----
    DMDHHSZA = ifelse(DMDHHSZA %in% c(9, 99, 999), NA, DMDHHSZA),  

    # ---- Income ----
    INDHHIN2 = case_when(
      INDHHIN2 %in% c(77,12,13, 99, 999) ~ NA, #make the under 20k and over 20k columns NA
      TRUE ~ as.character(INDHHIN2)
    ),
    
  )

```
### c.  
The Tympanometric width measure is looks approximately like a Poisson distribution. Fit four Poisson regression models predicting a respondent’s Tympanometric width in each ear. Each model is defined below, for a specific ear and a specific set of covariates.

1R - Right ear: gender
2R - Right ear: gender, citizenship status (as categorical), number of children (as continuous), annual household income (as continuous)
1L - Left ear: gender
2L - Left ear: gender, citizenship status (as categorical), number of children (as continuous), annual household income (as continuous)
Produce a table presenting the estimated incidence risk ratios for the coefficients in each model, along with the sample size for the model, the pseudo-
, and AIC values. (This can be a single table, or one table for coefficients and a separate table for model statistics).

The table(s) should be “nice” - use a function such as kable from knitr, or the stargazer package (or find another approach) to generate HTML/LaTeX tables for inclusion. The results should be clearly labeled, rounded appropriately, and easily readable.
```{r}
library(knitr)
library(pscl)  
library(dplyr)
#chatgpt taught me how to use ktable to present nicely

#1R
model_1 <- glm(AUXTWIDR~RIAGENDR, data = df_clean, family = "poisson",na.action = na.omit)
#2R
model_2 <- glm(AUXTWIDR~RIAGENDR+ DMDCITZN + DMDHHSZA + INDHHIN2, data = df_clean, family = "poisson",na.action = na.omit)
#1L
model_3 <- glm(AUXTWIDL~RIAGENDR, data = df_clean, family = "poisson",na.action = na.omit)
#2L
model_4 <- glm(AUXTWIDL~RIAGENDR+ DMDCITZN + DMDHHSZA + INDHHIN2, data = df_clean, family = "poisson",na.action = na.omit)

#this part of code is generated by chatgpt
extract_model_info <- function(model, name) {
  coefs <- coef(summary(model))
  
  # Incidence Rate Ratios (IRRs) and 95% CI
  irr <- exp(coef(model))
  ci <- exp(confint(model))
  
  # Model stats
  n <- nobs(model)
  pseudo_r2 <- pR2(model)["McFadden"]
  aic <- AIC(model)
  
  # Combine results
  data.frame(
    Model = name,
    Term = names(irr),
    IRR = round(irr, 3),
    CI_Lower = round(ci[, 1], 3),
    CI_Upper = round(ci[, 2], 3),
    Sample_Size = n,
    Pseudo_R2 = round(pseudo_r2, 3),
    AIC = round(aic, 1)
  )
}

# Apply to each model
results <- bind_rows(
  extract_model_info(model_1, "1R: Right Ear (Gender)"),
  extract_model_info(model_2, "2R: Right Ear (Full Model)"),
  extract_model_info(model_3, "1L: Left Ear (Gender)"),
  extract_model_info(model_4, "2L: Left Ear (Full Model)")
)

# Create a nice table
kable(results, caption = "Incidence Rate Ratios (IRRs) for Poisson Models",
      col.names = c("Model", "Variable", "IRR", "95% CI (Lower)", "95% CI (Upper)",
                    "N", "Pseudo-R² (McFadden)", "AIC"))

```
### d.   
From model 2L, provide evidence whether there is a difference between males and females in terms of their incidence risk ratio. Test whether the predicted value of Tympanometric width measure of the left ear differs between men and women. Include the results of the each test and their interpretation.
```{r}
#difference between males and females in irr
exp(coef(model_4)["RIAGENDR2"])

model_no_gender <- glm(AUXTWIDL ~ DMDCITZN + DMDHHSZA + INDHHIN2,
                       data = df_clean, family = "poisson", na.action = na.omit)
anova(model_no_gender, model_4, test = "Chisq")

```
*Answer:* The irr shows that females have an estimated 1.9% higher tympanometric width in the left ear than males, and the following anova test prove that the predicted value of Tympanometric width measure of the left ear differs between men and women.

## Problem 2
```{r}
#sql
library(DBI) 
library(RSQLite)
sakila <- dbConnect(RSQLite::SQLite(), "sakila_master.db")
```
### a.   
For each store, how many customers does that store have, and what percentage of customers of that store are active in the system?
```{r}
#sql+R
library(dplyr)
#get the customers table
customer <- dbGetQuery(sakila, "SELECT store_id, active FROM customer")
customer <- customer %>%
  mutate(active = as.numeric(active))
active_percentage <- customer %>%
  group_by(store_id) %>%
  summarise(
    total_customers = n(),
    active_customers = sum(active),
    percent_active = 100 * mean(active)
  )
active_percentage
```
```{r}
#sql
library(dplyr)
query <- "SELECT store_id, 
                 COUNT(*) AS total_customers,
                 COUNT(active) AS total, 
                 100.0 * AVG(active) AS percent_active
          FROM customer
          GROUP BY store_id; "
active_percentage <- dbGetQuery(sakila, query)
active_percentage
```
```{r}
#Comparison
library(microbenchmark)
microbenchmark(
  r_way = {
    customer <- dbGetQuery(sakila, "SELECT store_id, active FROM customer")
    customer <- customer %>%
      mutate(active = as.numeric(active))
    active_percentage <- customer %>%
      group_by(store_id) %>%
      summarise(
        total_customers = n(),
        active_customers = sum(active),
    percent_active = 100 * mean(active)
  )
  },
  sql_way = dbGetQuery(con, query),
  times = 50
)

```


### b.   
Generate a table identifying the names and country of each staff member.
```{r}
#sql+r
staff_query <- "SELECT s.first_name, s.last_name, a.address_id, a.city_id, c.country_id
          FROM staff AS s
          JOIN address AS a ON s.address_id = a.address_id
          JOIN city AS cty ON a.city_id = cty.city_id
          JOIN country AS c ON cty.country_id = c.country_id;"
staff <- dbGetQuery(sakila, staff_query)

staff_clean <- staff %>%
  mutate(full_name = paste(first_name, last_name)) %>%
  select(full_name, country_id)

staff_clean
```
```{r}
#sql 
#chatgpt told me how to paste the first name and last name together
staff_full_name_query <- "SELECT s.first_name || ' ' || s.last_name as full_name, c.country_id
          FROM staff AS s
          JOIN address AS a ON s.address_id = a.address_id
          JOIN city AS cty ON a.city_id = cty.city_id
          JOIN country AS c ON cty.country_id = c.country_id;"
staff_country <- dbGetQuery(sakila, staff_full_name_query)
staff_country
```
```{r}
#Comparison
microbenchmark(
  r_way = {
    staff <- dbGetQuery(sakila, staff_query)
    staff_clean <- staff %>%
      mutate(full_name = paste(first_name, last_name)) %>%
      select(full_name, country_id)
  },
  sql_way = dbGetQuery(sakila, staff_full_name_query),
  times = 50
)
```
### c.  
Identify the name(s) of the film(s) which was/were rented for the highest dollar value. (Assume all costs are in USD regardless of country.) (Hint: You can merge a table more than once.)
```{r}
#sql+r
film_query <- "SELECT f.title, p.amount
                FROM payment AS p
                JOIN rental AS r ON p.rental_id = r.rental_id
                JOIN inventory AS i ON r.inventory_id = i.inventory_id
                JOIN film AS f ON i.film_id = f.film_id;"
rented_film <- dbGetQuery(sakila, film_query)

film_clean <- rented_film %>%
                group_by(title) %>%
                  summarise(total_revenue = sum(amount)) %>%
                   filter(total_revenue == max(total_revenue))

film_clean
```
```{r}
#sql
film_value_query <- "SELECT f.title, sum(p.amount) as total_revenue
                FROM payment AS p
                JOIN rental AS r ON p.rental_id = r.rental_id
                JOIN inventory AS i ON r.inventory_id = i.inventory_id
                JOIN film AS f ON i.film_id = f.film_id
                ORDER BY total_revenue DESC
                LIMIT 1;"
film_value <- dbGetQuery(sakila, film_value_query)

film_clean
```
```{r}
#Comparison
microbenchmark(
  r_way = {
    film <- dbGetQuery(sakila, film_query)
    film_clean <- film %>%
                group_by(title) %>%
                  summarise(total_revenue = sum(amount)) %>%
                   filter(total_revenue == max(total_revenue))
  },
  sql_way = dbGetQuery(sakila, film_value_query),
  times = 50
)
```
## Problem 3   
```{r}
aus <- read.csv("au-500.csv")
```
### a.   
What percentage of the websites are .com’s (as opposed to .net, .com.au, etc)?
```{r}
com_ending <- (sum(grepl("\\.com(\\.|$)", aus$web))/nrow(aus))*100
com_ending
```
### b.   
What is the most common domain name amongst the email addresses? (In the email “statistics@umich.edu”, “umich.edu” is the domain name.)
```{r}
domain <- sub(".*@", "",aus$email)
sort(table(domain),decreasing = TRUE)[1]
```

### c.   
What proportion of company names contain a non-alphabetic character, excluding commas and whitespace. (E.g. “Jane Doe, LLC” would not contain an eligible non-alphabetic character; “Plumber 247” would.) What about if you also exclude ampersands (“&”)?
```{r}
non_alpha <- "[^a-zA-Z,[:space:]]"
non_alpha_amp <- "[^a-zA-Z,&[:space:]]"

non_alpha_prop <- (sum(grepl(non_alpha, aus$company))/nrow(aus))*100
non_alpha_amp_prop <- (sum(grepl(non_alpha_amp, aus$company))/nrow(aus))*100
non_alpha_prop
non_alpha_amp_prop
```
### d.   
In Australia, phone have 10 digits - but unlike in the US where we write all numbers as “123-456-7890”, they write land lines and cell phones differently1:

Landlines: 12-3456-7890
Cell phones: 1234-567-890
There are two different phones listed for each record. Make all phone numbers written like cell phones. Show it works by printing the first 10 phone numbers of each colum
```{r}
phone1_clean <- gsub("-","",aus$phone1)
phone1_cell <- paste0(substr(phone1_clean, 1, 4), "-", substr(phone1_clean, 5, 7),"-",substr(phone1_clean, 8, 10))
phone2_clean <- gsub("-","",aus$phone2)
phone2_cell <- paste0(substr(phone2_clean, 1, 4), "-", substr(phone2_clean, 5, 7),"-",substr(phone2_clean, 8, 10))
head(phone1_cell,10)
head(phone2_cell,10)
```
### e.   
Produce a histogram of the log of the apartment numbers for all addresses. (You may assume any number at the end of the an address is an apartment number.)
```{r}
library(ggplot2)
#the data that truly ends with apt numbers
apt <- aus[grepl("#[0-9]*$", aus$address), ]
apt$apt_num <- as.numeric(sub(".*?(\\d+)$", "\\1", apt$address))
hist(log(apt$apt_num), main = "Histogram of log10(apartment numbers)" ,xlab ="log10(apartment number)" )
```
### f.  
Benford’s law is an observation about the distribution of the leading digit of real numerical data. Examine whether the apartment numbers appear to follow Benford’s law. Do you think the apartment numbers would pass as real data?
```{r}
#chatgpt taught me how to plot the barplots together for comparison
leading <- as.numeric(substr(as.character(apt_nums), 1, 1))
probs <- table(leading) / length(leading)
benford <- log10(1 + 1 / (1:9))

barplot(rbind(probs, benford),
        beside = TRUE, col = c("blue", "red"),
        names.arg = 1:9,
        legend.text = c("Observed", "Benford Expected"),
        args.legend = list(x = "topright"))
```
*Answer:* The plot shows that the aus data do not follow the Benford rules and the difference is quite obvious. However if it is the real data I think it would pass, as according to the wikipedia apartment number is one scenario that Benford rules usually apply.




